{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automated-mercy",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "This section contains the boilerplate necessary for the other sections. Run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hungarian-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets_.coco import build, CocoDetection\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from util.misc import nested_tensor_from_tensor_list\n",
    "import skimage\n",
    "import colorsys\n",
    "import random\n",
    "from skimage.measure import find_contours\n",
    "from matplotlib.patches import Polygon\n",
    "from skimage import io\n",
    "import argparse\n",
    "import datasets_.transforms as T\n",
    "import copy\n",
    "import glob\n",
    "import re\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unknown-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legitimate-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coco_transforms(image_set, args):\n",
    "\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    scales = [800]\n",
    "\n",
    "    if image_set == 'val' or image_set == 'train':\n",
    "        return T.Compose([\n",
    "            T.RandomResize([scales[-1]], max_size=scales[-1] * 1333 // 800),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "def plot_gt(im, labels, bboxes_scaled, output_dir):\n",
    "    tl = 3\n",
    "    tf = max(tl-1, 1)\n",
    "    tempimg = copy.deepcopy(im)\n",
    "    color = [255,0,0]\n",
    "    for label, (xmin, ymin, xmax, ymax) in zip(labels.tolist(), bboxes_scaled.tolist()):\n",
    "        c1, c2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
    "        cv2.rectangle(tempimg, c1, c2, color, tl, cv2.LINE_AA)\n",
    "        text = f'{CLASSES[label]}'\n",
    "        t_size = cv2.getTextSize(text, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(tempimg, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(tempimg, text, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    fname = os.path.join(output_dir,'gt_img.png')\n",
    "    cv2.imwrite(fname, tempimg)\n",
    "    print(f\"{fname} saved.\")\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "def draw_bbox_in_img(fname, bbox_scaled, score, color=[0,255,0]):\n",
    "    tl = 3\n",
    "    tf = max(tl-1,1) # font thickness\n",
    "    # color = [0,255,0]\n",
    "    im = cv2.imread(fname)\n",
    "    for p, (xmin, ymin, xmax, ymax) in zip(score, bbox_scaled.tolist()):\n",
    "        c1, c2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
    "        cv2.rectangle(im, c1, c2, color, tl, cv2.LINE_AA)\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        t_size = cv2.getTextSize(text, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(im, text, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    cv2.imwrite(fname, im)\n",
    "\n",
    "def plot_results(cv2_img, prob, boxes, output_dir):\n",
    "    tl = 3 # thickness line\n",
    "    tf = max(tl-1,1) # font thickness\n",
    "    tempimg = copy.deepcopy(cv2_img)\n",
    "    color = [0,0,255]\n",
    "    for p, (xmin, ymin, xmax, ymax) in zip(prob, boxes.tolist()):\n",
    "        c1, c2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
    "        cv2.rectangle(tempimg, c1, c2, color, tl, cv2.LINE_AA)\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        t_size = cv2.getTextSize(text, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(tempimg, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(tempimg, text, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    fname = os.path.join(output_dir,'pred_img.png')\n",
    "    cv2.imwrite(fname, tempimg)\n",
    "    print(f\"{fname} saved.\")\n",
    "    \n",
    "def increment_path(path, exist_ok=False, sep='', mkdir=False):\n",
    "    # Increment file or directory path, i.e. runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... etc.\n",
    "    path = Path(path)  # os-agnostic\n",
    "    if path.exists() and not exist_ok:\n",
    "        suffix = path.suffix\n",
    "        path = path.with_suffix('')\n",
    "        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n",
    "        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]  # indices\n",
    "        n = max(i) + 1 if i else 2  # increment number\n",
    "        path = Path(f\"{path}{sep}{n}{suffix}\")  # update path\n",
    "    dir = path if path.suffix == '' else path.parent  # directory\n",
    "    if not dir.exists() and mkdir:\n",
    "        dir.mkdir(parents=True, exist_ok=True)  # make directory\n",
    "    return path\n",
    "\n",
    "def save_pred_fig(output_dir, output_dic, keep):\n",
    "    # im = Image.open(os.path.join(output_dir, \"img.png\"))\n",
    "    im = cv2.imread(os.path.join(output_dir, \"img.png\"))\n",
    "    h, w = im.shape[:2]\n",
    "    bboxes_scaled = rescale_bboxes(output_dic['pred_boxes'][0, keep].cpu(), (w,h))\n",
    "    prob = output_dic['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    scores = prob[keep]\n",
    "    plot_results(im, scores, bboxes_scaled, output_dir)\n",
    "\n",
    "def save_gt_fig(output_dir, gt_anno):\n",
    "    im = cv2.imread(os.path.join(output_dir, \"img.png\"))\n",
    "    h, w = im.shape[:2]\n",
    "    bboxes_scaled = rescale_bboxes(gt_anno['boxes'], (w,h))\n",
    "    labels = gt_anno['labels']\n",
    "    plot_gt(im, labels, bboxes_scaled, output_dir)\n",
    "\n",
    "def get_one_query_meanattn(vis_attn,h_featmap,w_featmap):\n",
    "    mean_attentions = vis_attn.mean(0).reshape(h_featmap, w_featmap)\n",
    "    mean_attentions = nn.functional.interpolate(mean_attentions.unsqueeze(0).unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
    "    return mean_attentions\n",
    "\n",
    "def get_one_query_attn(vis_attn, h_featmap, w_featmap, nh):\n",
    "    attentions = vis_attn.reshape(nh, h_featmap, w_featmap)\n",
    "    # attentions = vis_attn.sum(0).reshape(h_featmap, w_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-suspension",
   "metadata": {},
   "source": [
    "## Set args\n",
    "set some args for loading model and saving visualization:\n",
    "- --patch_size should align with load model setting\n",
    "- --project set the path, where visualization save\n",
    "- --name dont change default value\n",
    "- --index set the img index in coco train/val split\n",
    "- --backbone_name should align with load model setting\n",
    "- --coco_path set coco dataset path\n",
    "- --resume set load model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vietnamese-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Visualize Self-Attention maps', add_help=False)\n",
    "    parser.add_argument('--patch_size', default=16, type=int, help='Patch resolution of the model.')\n",
    "    parser.add_argument('--project', default='./visualization_new', help='Path where to save visualizations.')\n",
    "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "    parser.add_argument('--index', default=5, type=int, help='index of dataset')\n",
    "    parser.add_argument('--backbone_name', default='tiny', type=str,\n",
    "                        help=\"Name of the deit backbone to use\")\n",
    "    parser.add_argument('--coco_path', default='/data/qingsong/dataset/coco', type=str,\n",
    "                        help=\"split\")\n",
    "    parser.add_argument('--image_set', default='val', type=str,\n",
    "                        help=\"split\")\n",
    "    parser.add_argument('--pre_trained', default='',\n",
    "                        help=\"set imagenet pretrained model path if not train yolos from scatch\")\n",
    "    parser.add_argument(\"--det_token_num\", default=100, type=int,\n",
    "                        help=\"Number of det token in the deit backbone\")\n",
    "    parser.add_argument('--init_pe_size', nargs='+', type=int, default=[512,864],\n",
    "                        help=\"init pe size (h,w)\")\n",
    "    parser.add_argument('--mid_pe_size', nargs='+', type=int, default=[512,864],\n",
    "                        help=\"mid pe size (h,w)\")\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint') \n",
    "    return parser\n",
    "parser = argparse.ArgumentParser('Visualize Self-Attention maps', parents=[get_args_parser()])\n",
    "args = parser.parse_args(\"\")\n",
    "args.output_dir = str(increment_path(Path(args.project) / args.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-poverty",
   "metadata": {},
   "source": [
    "## load model & coco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "global rank 0 is loading checkpoint /data/qingsong/pretrain/yolos/1/mp_rank_00_model_states.pt\n",
      "  successfully loaded /data/qingsong/pretrain/yolos/1/mp_rank_00_model_states.pt\n",
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# pretrain_path = '/data/qingsong/pretrain'\n",
    "\n",
    "# import torch\n",
    "# from models.detector import Detector\n",
    "# yolos = Detector(\n",
    "#         num_classes=91,\n",
    "#         pre_trained=None,\n",
    "#         det_token_num=100,\n",
    "#         backbone_name='tiny',\n",
    "#         init_pe_size=[800, 1333],\n",
    "#         mid_pe_size=[512, 864],\n",
    "#         use_checkpoint=False,\n",
    "#     )\n",
    "# yolos.load_state_dict(torch.load(os.path.join(pretrain_path, 'yolos_ti.pth'))['model'], strict=False)\n",
    "\n",
    "\n",
    "swiss_args = argparse.Namespace(\n",
    "    num_layers=12,\n",
    "    vocab_size=101,\n",
    "    num_det_tokens=100,\n",
    "    hidden_size=192,\n",
    "    num_attention_heads=3,\n",
    "    hidden_dropout=0.,\n",
    "    attention_dropout=0.,\n",
    "    in_channels=3,\n",
    "    image_size=[512, 864],\n",
    "    patch_size=16,\n",
    "    pre_len=1,\n",
    "    post_len=100,\n",
    "    inner_hidden_size=None,\n",
    "    hidden_size_per_attention_head=None,\n",
    "    checkpoint_activations=False,\n",
    "    checkpoint_num_layers=1,\n",
    "    sandwich_ln=False,\n",
    "    post_ln=False,\n",
    "    model_parallel_size=1,\n",
    "    world_size=1,\n",
    "    rank=0,\n",
    "    num_classes=1000,\n",
    "    load='/data/qingsong/pretrain/yolos',\n",
    "    old_image_size=[800, 1333],\n",
    "    old_pre_len=1,\n",
    "    old_post_len=100,\n",
    "    old_checkpoint=None,\n",
    "    mode='inference',\n",
    "    num_det_classes=92\n",
    "    )\n",
    "\n",
    "import os\n",
    "import torch\n",
    "init_method = 'tcp://'\n",
    "master_ip = os.getenv('MASTER_ADDR', '127.0.0.1')\n",
    "master_port = os.getenv('MASTER_PORT', '12468')\n",
    "init_method += master_ip + ':' + master_port\n",
    "torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        world_size=swiss_args.world_size, rank=swiss_args.rank, init_method=init_method)\n",
    "import SwissArmyTransformer.mpu as mpu\n",
    "mpu.initialize_model_parallel(swiss_args.model_parallel_size)\n",
    "from yolos_model import YOLOS\n",
    "from SwissArmyTransformer.training.deepspeed_training import load_checkpoint\n",
    "swiss_model = YOLOS(swiss_args)\n",
    "load_checkpoint(swiss_model, swiss_args)\n",
    "swiss_model.get_mixin('pos_embedding').reinit() # patch_embedding should not reinit for inference\n",
    "model = swiss_model\n",
    "\n",
    "root = Path(args.coco_path)\n",
    "assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "mode = 'instances'\n",
    "image_set=args.image_set\n",
    "PATHS = {\n",
    "    \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n",
    "    \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n",
    "}\n",
    "img_folder, ann_file = PATHS[image_set]\n",
    "dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set, None), return_masks=False)\n",
    "img_data, img_anno = dataset.__getitem__(args.index)\n",
    "ret=nested_tensor_from_tensor_list(img_data.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-technician",
   "metadata": {},
   "source": [
    "## forward to get pred & attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becoming-messenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/qingsong/anaconda3/envs/swiss/lib/python3.8/site-packages/torch/nn/functional.py:3499: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualization_new/exp/pred_img.png saved.\n",
      "visualization_new/exp/gt_img.png saved.\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.eval()\n",
    "model.to(device)\n",
    "ret = ret.to(device)\n",
    "\n",
    "\n",
    "images = ret.tensors\n",
    "batch_size, _, height, width = images.shape\n",
    "num_patches = (height//16) * (width//16)\n",
    "seq_len = 1 + num_patches + model.get_mixin('det_head').num_det_tokens\n",
    "position_ids = torch.cat([torch.arange(seq_len)[None,]]*batch_size)\n",
    "encoded_input = {'input_ids':torch.cat([torch.arange(1+model.get_mixin('det_head').num_det_tokens)[None,]]*batch_size).long(), 'image':images, 'position_ids':position_ids}\n",
    "encoded_input = {k:v.to(device) for k,v in encoded_input.items()}\n",
    "encoded_input['attention_mask'] = None\n",
    "\n",
    "outputs = model(**encoded_input, offline=False, height=height//16, width=width//16)[0]\n",
    "\n",
    "# outputs = yolos(ret)\n",
    "\n",
    "# attention = model.forward_return_attention(ret)\n",
    "# attention = attention[-1].detach().cpu()\n",
    "# nh = attention.shape[1] # number of head\n",
    "# attention = attention[0, :, -args.det_token_num:, 1:-args.det_token_num]\n",
    "#forward input to get pred\n",
    "result_dic = outputs\n",
    "# get visualize dettoken index\n",
    "probas = result_dic['pred_logits'].softmax(-1)[0, :, :-1].cpu()\n",
    "keep = probas.max(-1).values > 0.9\n",
    "vis_indexs = torch.nonzero(keep).squeeze(1)\n",
    "# save original image\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "img = ret.tensors.squeeze(0).cpu()\n",
    "torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(args.output_dir, \"img.png\"))\n",
    "\n",
    "# save pred image\n",
    "save_pred_fig(args.output_dir, result_dic, keep)\n",
    "\n",
    "# save gt image\n",
    "save_gt_fig(args.output_dir, img_anno)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
